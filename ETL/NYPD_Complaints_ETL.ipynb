{"cells":[{"cell_type":"markdown","source":["# Part 1 - Initial Cleaning and Transforming of the Complaints Data"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"30c97cd1-b2ce-4908-8a3a-b787c8cf79db","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["The following ETL section was done in VScode using pandas. \\\nRead in file containing complaints dataset from NYC Open Data Website \nhttps://data.cityofnewyork.us/Public-Safety/NYPD-Complaint-Data-Historic/qgea-i56i"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9de8da33-7095-46b8-a09f-889a40c98111","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["crime = pd.read_csv('NYPD_Complaint_Data_Historic.csv', low_memory = False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0e109e60-296b-434b-8614-2be2d3066fa9","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Separate date to day, month and year to replace incorrect dates"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b2219e71-4035-450e-bbd1-c57a5d9d62f1","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["crime[['DAY','MONTH','YEAR']] = crime['CMPLNT_FR_DT'].str.split('/', 2, expand = True)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a55db32a-a92d-4055-9aa6-a94f77294fc4","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Replace incorrectly entered years"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a565fa5d-c90a-4664-b103-45e7c7ac3312","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["crime['YEAR'] = crime['YEAR'].replace(['1028'], '2018')\ncrime['YEAR'] = crime['YEAR'].replace(['1017'], '2017')\ncrime['YEAR'] = crime['YEAR'].replace(['1018'], '2018')\ncrime['YEAR'] = crime['YEAR'].replace(['1016'], '2016')\ncrime['YEAR'] = crime['YEAR'].replace(['1027'], '2017')\ncrime['YEAR'] = crime['YEAR'].replace(['1015'], '2015')\ncrime['YEAR'] = crime['YEAR'].replace(['1025'], '2015')\ncrime['YEAR'] = crime['YEAR'].replace(['1021'], '2021')\ncrime['YEAR'] = crime['YEAR'].replace(['1019'], '2019')\ncrime['YEAR'] = crime['YEAR'].replace(['1029'], '2019')\ncrime['YEAR'] = crime['YEAR'].replace(['1010'], '2020')\ncrime['YEAR'] = crime['YEAR'].replace(['1020'], '2020')\ncrime['YEAR'] = crime['YEAR'].replace(['1026'], '2016')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d73304c2-561f-4a88-8908-8732a37e5141","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Recombine day, month and year to date"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9e1e6a7e-ef80-43dd-bdf8-ee97a93c7c04","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["crime['DATE'] = crime[['DAY','MONTH','YEAR']].apply(lambda row: '/'.join(row.values.astype(str)), axis=1)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9b26e21f-d1da-474f-9992-12b5b09de9ce","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Drop previously created columns and column with incorrect dates"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f1ed7642-6aee-4173-bbed-7ba17a254526","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["crime = crime.drop(['CMPLNT_FR_DT','DAY','MONTH','YEAR'], axis = 1)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3231d54d-f60c-424b-b4e7-87e04b2d0389","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Convert time related columns to datetime, then create a combined date and time column - This will take a while to run"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3531c660-0142-408d-93cd-1a83e0ff9cce","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["import datetime\nfrom datetime import date, time\ncrime = crime.astype({'DATE':str, 'CMPLNT_FR_TM':str})\ncrime['DATE_AND_TIME'] = pd.to_datetime(crime['DATE'] + ' ' + crime['CMPLNT_FR_TM'], errors = 'coerce')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0a96b82d-300b-4977-b7f6-9270d0dbfee0","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Convert date column to datetime as well"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"87f8dbaa-bd99-49e4-a25f-b7bf787dc6eb","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["crime['DATE'] = pd.to_datetime(crime['DATE'], format='%m/%d/%Y')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fe521312-b8e2-49f2-8950-c117f04b5887","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Sort to years 2017 and after"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2b737009-9451-40c1-99a2-301d443ec78d","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["crime = crime[crime['DATE_AND_TIME'] >= '2017/01/01']"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a9b70652-fd7d-4a57-9498-3dd1e9e256e2","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Remove rows with null coordinates"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"288ff06f-dec5-4ab7-9de4-08285e045a38","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["crime = crime.dropna(subset = ['Latitude'])\ncrime = crime.dropna(subset = ['Longitude'])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5dfae498-52ba-45ac-b039-f334feab91b6","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Export to csv in a folder for exported data - Uploaded to a blob in a storage container to pull from later"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fb216e12-e1ba-46ca-8fa6-9364eb262ae5","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["crime.to_csv(r'C:\\Users\\{file_path}\\Exports\\crime.csv', header=True)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e507543f-1aec-4cc1-ab91-8f6a08a926d0","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Part 2 - Further Cleaning of the Complaints Data"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"afbbfcda-d7a1-410e-867d-cee4483418e9","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["import pandas as pd\nimport datetime\nimport re\n\nSAS_TOKEN = 'sp=racwdlmeop&st=2023-01-19T15:17:20Z&se=2023-02-10T23:17:20Z&spr=https&sv=2021-06-08&sr=c&sig=SNP1pr7qFgO1k1a8nm2MmfX9mp2EnPJKaBQ7eHEEgsg%3D'\nCONTAINER = 'fg4'\nSTOR_ACCT = 'cohort40storage'\nROOT_PATH = f'wasbs://{CONTAINER}@{STOR_ACCT}.blob.core.windows.net/'\n\nspark.conf.set(f'fs.azure.sas.{CONTAINER}.{STOR_ACCT}.blob.core.windows.net', SAS_TOKEN)\n\nread_path = ROOT_PATH + 'mta-nypd/crime.csv'\n# df = spark.read.format('csv').option('header',True).load(read_path)\nall_nyc_complaints_spark = spark.read.csv(\n    read_path, \n    header=True, \n    mode=\"DROPMALFORMED\", \n    multiLine = True\n)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1bd932f7-ef80-4ef1-a4cd-8b14edaa472e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The code above provides access to crime.csv file from a storage blob. This file is imported as a pyspark dataframe and covers complaints from 2017 to 2021 and in all five boroughs. Because this is such a large dataset, we will be breaking it down to borough and year. First, we remove columns that were deemed to be unnecessary, such as the park's name (if the crime occured at a park), using the code below."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2a0ca325-fe2f-45ae-8af0-feeec2057f34","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["nyc_complaints_spark = all_nyc_complaints_spark.drop('CMPLNT_FR_TM', 'CMPLNT_TO_DT', 'CMPLNT_TO_TM', 'CRM_ATPT_CPTD_CD', 'JURISDICTION_CODE', 'PARKS_NM', 'HADEVELOPT', 'HOUSING_PSA', 'X_COORD_CD', 'Y_COORD_CD', 'TRANSIT_DISTRICT', 'Lat_Lon')\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d213bdb2-08ed-462a-b0a8-92f904b9941c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The large dataset is broken down to smaller subsets since it was too large for pandas dataframe. Randomsplit was used to distribute the rows equally into 10 subsets."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c758f1f3-9387-4a6e-a66d-925dc74d1bb4","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df1,df2,df3,df4,df5,df6,df7,df8,df9,df10 = nyc_complaints_spark.randomSplit([0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1], seed = 0)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"11dc3f3b-ba98-454c-b7ce-94b46c61c027","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def only_borough(df):\n    global BK\n    global MN\n    global BX\n    global QN\n    global SI\n    global NA\n    complaint_BK = df[df['BORO_NM'] == 'BROOKLYN']\n    BK = pd.concat([BK, complaint_BK])\n    complaint_MN = df[df['BORO_NM'] == 'MANHATTAN']\n    MN = pd.concat([MN, complaint_MN])\n    complaint_QN = df[df['BORO_NM'] == 'QUEENS']\n    QN = pd.concat([QN, complaint_QN])    \n    complaint_SI = df[df['BORO_NM'] == 'STATEN ISLAND']\n    SI = pd.concat([SI, complaint_SI])\n    complaint_BX = df[df['BORO_NM'] == 'BRONX']\n    BX = pd.concat([BX, complaint_BX])\n    complaint_NA = df[pd.isnull(df['BORO_NM'])]\n    NA = pd.concat([NA, complaint_NA])\n\nBK = pd.DataFrame()\nMN = pd.DataFrame()\nQN = pd.DataFrame()\nSI = pd.DataFrame()\nBX = pd.DataFrame()\nNA = pd.DataFrame()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"dfe03b8e-ff40-4bd9-9d6c-913373235902","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Six new dataframes were created, one for each borough and one for rows that did not name a borough. The helper function above filters out each row into its respective borough. Below is the actual iteration through all 10 subsets. The resulting six dataframes were converted back into a ppyspark dataframe and saved as csvs."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8d2c8c58-2aac-433e-8cd7-74bbd76c8831","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["data = [df1, df2, df3, df4, df5, df6, df7, df8, df9, df10]    \nboroughs = [MN, QN, BK, SI, NA, BX]\nbor = ['MN', 'QN', 'BK', 'SI', 'NA', 'BX']\n    \nfor i in data:    \n    only_borough(pd_df)\n    \nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType, DateType\nmySchema = StructType([ StructField(\"CMPLNT_NUM\", StringType(), True)\\\n                       ,StructField(\"ADDR_PCT_CD\", StringType(), True)\\\n                       ,StructField(\"RPT_DT\", StringType(), True)\\\n                       ,StructField(\"KY_CD\", StringType(), True)\\\n                       ,StructField(\"OFNS_DESC\", StringType(), True)\\\n                       ,StructField(\"PD_CD\", StringType(), True)\\\n                       ,StructField(\"PD_DESC\", StringType(), True)\\\n                       ,StructField(\"LAW_CAT_CD\", StringType(), True)\\\n                       ,StructField(\"BORO_NM\", StringType(), True)\\\n                       ,StructField(\"LOC_OF_OCCUR_DESC\", StringType(), True)\\\n                       ,StructField(\"PREM_TYP_DESC\", StringType(), True)\\\n                       ,StructField(\"JURIS_DESC\", StringType(), True)\\\n                       ,StructField(\"SUSP_AGE_GROUP\", StringType(), True)\\\n                       ,StructField(\"SUSP_RACE\", StringType(), True)\\\n                       ,StructField(\"SUSP_SEX\", StringType(), True)\\\n                       ,StructField(\"Latitude\", StringType(), True)\\\n                       ,StructField(\"Longitude\", StringType(), True)\\\n                       ,StructField(\"PATROL_BORO\", StringType(), True)\\\n                       ,StructField(\"STATION_NAME\", StringType(), True)\\\n                       ,StructField(\"VIC_AGE_GROUP\", StringType(), True)\\\n                       ,StructField(\"VIC_RACE\", StringType(), True)\\\n                       ,StructField(\"VIC_SEX\", StringType(), True)\\\n                       ,StructField(\"DATE\", StringType(), True)\\\n                       ,StructField(\"DATE_AND_TIME\", StringType(), True)])\n\nfor i in range(5):\n    subway = spark.createDataFrame(borough[i],schema=mySchema)\n    subway.coalesce(1).write.mode('overwrite').csv(ROOT_PATH + f\"/{bor[i]}_complaints.csv\", header = 'True')\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6b5514ea-f446-4fc5-930d-e0601c421fd1","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#Part 3 - Transformations: Connect Complaints to Stations"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"094d9ab0-c05e-43eb-8b1c-167fa80e582c","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["spark.conf.set(f'fs.azure.sas.{CONTAINER}.{STOR_ACCT}.blob.core.windows.net', SAS_TOKEN)\n\nread_path = ROOT_PATH + 'BX_complaints.csv'\nBX_complaints_spark = spark.read.csv(\n    read_path, \n    header=True, \n    mode=\"DROPMALFORMED\", \n    inferSchema = True,\n    multiLine = True\n)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0a7405e9-782b-4d01-91dd-d00f6c519076","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["At this point, each of the csvs we saved only contains complaints from one borough. The Bronx dataset is shown above. It is worth noting that the inferred imported schema has changed the last column to a timestamp, which will come in handy when the rows are further separated out into years. The code above was also altered so that it can also read in the datasets for the other boroughs. The first helper function below filters out rows that are between two dates and the second saves the rows as a new csv."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"73155ccb-21d8-4e6d-bf92-3ff339a24910","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql import functions as F\n\ndef only_year(old_df, first_date, second_date):\n    return old_df\\\n        .filter((F.col('DATE') > F.lit(first_date)) &\\\n        (F.col('DATE') < F.lit(second_date)))\n\ndef save(df, borough, year):\n    df_name = spark.createDataFrame(df)\n    df_name.coalesce(1).write.mode('overwrite').csv(ROOT_PATH + f\"/Borough_Year_Complaints/{borough}_{year}.csv\", header = 'True')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0d78c00a-93a2-4e2a-b7f2-89eea3ea4725","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The only_year function was used to separate the datasets of each borough into different years, ranging from 2017 to 2021. Below is an example of how the Manhattan crimes were separated out."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ec6c7cbc-e5a2-49ca-bb4a-6d0e3197897e","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["MN_2021 = only_year(MN_complaints_spark, '2020-12-31', '2022-01-01')\nMN_2020 = only_year(MN_complaints_spark, '2019-12-31', '2021-01-01')\nMN_2019 = only_year(MN_complaints_spark, '2018-12-31', '2020-01-01')\nMN_2018 = only_year(MN_complaints_spark, '2017-12-31', '2019-01-01')\nMN_2017 = only_year(MN_complaints_spark, '2016-12-31', '2018-01-01')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2b05251f-c47d-48f3-adec-9a3649208a26","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["A dataset regarding the locations of the train stations was imported in as a pyspark dataframe in the cell below. This dataset will help us determine the closest train station to a complaint and what the distance is. It should be noted that the train stations dataset does not include Staten Island train stations."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"134899f7-7fa4-4444-b89f-ad16bbf13218","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["read_path = ROOT_PATH + 'mta-nypd/stopsNYCgrouped.csv'\nstations = spark.read.csv(\n    read_path, \n    header=True,\n    inferSchema = True,\n    mode=\"DROPMALFORMED\", \n    multiLine = True)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ddc833e1-a44f-4a38-b2d6-82bc9e862057","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["stations = stations.withColumnRenamed('Station Name','Station_Name').withColumnRenamed('Station Latitude','Station_Latitude').withColumnRenamed('Station Longitude','Station_Longitude').withColumnRenamed('All Lines','All_Lines')\n\ns = stations.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0c75cbd0-cf0e-4cb6-8ca5-bb3b71266503","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Two more helper functions below. The distance function was designed to determine the distance between two locations using latitudes and longitudes. Using the distance function, the closest_station function would iterate through all the stations' location and the location of a complaint to determine which station is the closest."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d7fb8c36-1a81-41ca-b0a5-2d85c34b31f1","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from math import cos, asin, sqrt, pi\n\ndef distance(lat1, lon1, lat2, lon2):\n    p = pi/180    \n    a = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p) * cos(lat2*p) * (1-cos((lon2-lon1)*p))/2    \n    return 12742 * asin(sqrt(a))\n\ndef closest_station(lat, long, stations):\n    min_distance = 10000000.0    \n    closest = ''    \n    for i in range(465):\n        tmp = distance(\n                    lat,\n                    long,\n                    stations[i].Station_Latitude,\n                    stations[i].Station_Longitude)\n        if tmp < min_distance:\n            min_distance = tmp            \n            closest = stations[i].Station_Name   \n            line = stations[i].All_Lines\n    return closest, min_distance, line"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9f711fea-8487-40e2-a94d-3a92e831e41b","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The code below changes each pyspark dataframe to a pandas dataframe, loops through each dataset for a borough and adds on a column for the closest station, the distance between the crime and the station, and all the trains that stops at that station. The dataframes are then saved to csv files.  Similar to the code previous, the one below was altered for each borough by changing out the two letter code the borough."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4a539451-6b10-42fc-9bef-7606f9de39cb","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["datas = [SI_2021, SI_2020, SI_2019, SI_2018, SI_2017]\nyear = 2021\nfor data in datas:\n    data = data.drop('STATION_NAME')\n    pd_data = data.toPandas()\n    pd_data['closest_station'] = pd_data.apply(\n                                   lambda row:\n                                   closest_station(\n                                                    row['Latitude'],\n                                                    row['Longitude'],\n                                                    s),\n                                   axis=1)\n    pd_data[['closest_station','station_distance','station_line']] = pd.DataFrame(\n    pd_data['closest_station'].tolist(),\n    index = pd_data.index)\n    save(pd_data, 'Staten_Island', year)\n    print(f'{year} for Staten Island data saved')\n    year -=1\n    \n    "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"99d14612-2258-444b-a7c7-bf2558592985","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"2021 for Staten Island data saved\n2020 for Staten Island data saved\n2019 for Staten Island data saved\n2018 for Staten Island data saved\n2017 for Staten Island data saved\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["2021 for Staten Island data saved\n2020 for Staten Island data saved\n2019 for Staten Island data saved\n2018 for Staten Island data saved\n2017 for Staten Island data saved\n"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"951f9701-27b6-4801-8f4b-4c83c277dc89","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.9.13","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"orig_nbformat":4,"application/vnd.databricks.v1+notebook":{"notebookName":"NYPD_Complaints_ETL","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2814743111359695},"vscode":{"interpreter":{"hash":"1643639e6d89f06a3b88736f47d07c6c9b55a5b70383cc3272fa94752bd05ca9"}}},"nbformat":4,"nbformat_minor":0}
